# CodeChunking

A production-grade semantic code search system using Go, PostgreSQL with pgvector, NATS JetStream, and Google Gemini. Built with hexagonal architecture and comprehensive TDD methodology.

ðŸŽ‰ **MVP Status: COMPLETE** - Full semantic search functionality with natural language queries!

## Features

### Core Functionality
- **Repository Indexing**: Clone and index Git repositories automatically
- **Intelligent Code Chunking**: Parse code into semantic units using tree-sitter
- **Vector Embeddings**: Generate embeddings with Google Gemini API
- **Semantic Code Search**: Natural language queries with POST /search endpoint âœ¨
- **Advanced Filtering**: Filter by repository, language, file type, and similarity threshold
- **Pagination & Sorting**: Full pagination with multiple sorting options (similarity, file path)
- **Asynchronous Processing**: Scalable job processing with NATS JetStream

### Production Features
- **High-Performance Messaging**: NATS JetStream client with 305,358+ msg/sec throughput
- **Advanced Health Monitoring**: 23.5Âµs average response time with intelligent caching
- **Circuit Breaker Patterns**: Connection resilience and fault tolerance
- **Structured Logging**: Correlation ID tracking with cross-component tracing
- **Comprehensive Security**: Input validation, XSS prevention, SQL injection protection
- **Robust Error Handling**: Centralized panic recovery and validation

### Architecture & Development
- **Hexagonal Architecture**: Clean, maintainable code structure with clear separation
- **TDD Implementation**: Using specialized red-green-refactor agent methodology
- **CLI Interface**: Cobra-based CLI with multiple commands
- **Configuration Management**: Hierarchical configuration with Viper

## Architecture

The system follows hexagonal architecture (ports and adapters) principles:

```
â”œâ”€â”€ cmd/                    # CLI commands (Cobra)
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ domain/            # Core business logic
â”‚   â”œâ”€â”€ application/       # Use cases
â”‚   â”œâ”€â”€ port/             # Interface definitions
â”‚   â””â”€â”€ adapter/          # Interface implementations
â”œâ”€â”€ configs/              # Configuration files
â”œâ”€â”€ migrations/           # Database migrations
â””â”€â”€ docker/              # Docker configurations
```

## Data Flow Architecture

The following diagram illustrates how repositories are processed, chunked, stored, and retrieved:

```mermaid
graph TB
    %% User Interface Layer
    subgraph "Client Layer"
        USER[User/CLI]
        API_CLIENT[API Client]
    end

    %% API Layer
    subgraph "API Layer (Inbound Adapters)"
        API_SERVER[API Server :8080]
        REPO_HANDLER[Repository Handler]
        SEARCH_HANDLER[Search Handler]
        HEALTH[Health Check]
    end

    %% Application Layer
    subgraph "Application Layer"
        REPO_SERVICE[Repository Service]
        SEARCH_SERVICE[Search Service]
        WORKER_SERVICE[Worker Service]
        JOB_PROCESSOR[Job Processor]
    end

    %% Domain Layer
    subgraph "Domain Layer"
        REPO_ENTITY[Repository Entity]
        JOB_ENTITY[IndexingJob Entity]
        ALERT_ENTITY[Alert Entity]
        ERROR_ENTITY[ClassifiedError Entity]
    end

    %% Message Queue
    subgraph "Messaging (NATS JetStream)"
        NATS_PUBLISHER[Message Publisher]
        NATS_STREAM[INDEXING Stream]
        NATS_CONSUMER[Message Consumer]
    end

    %% Processing Pipeline
    subgraph "Background Processing Pipeline"
        GIT_CLONE[Git Repository<br/>Clone]
        TS_PARSER[Tree-sitter<br/>Parser Pool]

        subgraph "Language Parsers"
            GO_PARSER[Go Parser]
            PY_PARSER[Python Parser]
        end

        CHUNKER[Function/Class<br/>Chunker]
        SEMANTIC_ANALYZER[Semantic Analysis<br/>Service]
    end

    %% Embedding & Storage
    subgraph "Embedding & Vector Storage"
        GEMINI_CLIENT[Google Gemini<br/>Embedding Client]
        BATCH_PROCESSOR[Parallel Batch<br/>Processor]
        RETRY_LOGIC[Retry Logic &<br/>Circuit Breaker]
    end

    %% Data Storage
    subgraph "Data Storage (Outbound Adapters)"
        POSTGRES[(PostgreSQL<br/>Database)]
        PGVECTOR[(pgvector<br/>Extension)]

        subgraph "Database Tables"
            REPO_TABLE[(repositories)]
            CHUNKS_TABLE[(code_chunks)]
            EMBED_TABLE[(embeddings)]
            EMBED_PART[(embeddings_partitioned)]
            JOBS_TABLE[(indexing_jobs)]
        end
    end

    %% Request Flow - Repository Creation
    USER -->|POST /repositories| API_SERVER
    API_SERVER --> REPO_HANDLER
    REPO_HANDLER --> REPO_SERVICE
    REPO_SERVICE --> REPO_ENTITY
    REPO_SERVICE -->|Save Repository| POSTGRES
    REPO_SERVICE -->|Publish Job| NATS_PUBLISHER
    NATS_PUBLISHER --> NATS_STREAM

    %% Background Processing Flow
    NATS_STREAM --> NATS_CONSUMER
    NATS_CONSUMER --> WORKER_SERVICE
    WORKER_SERVICE --> JOB_PROCESSOR

    JOB_PROCESSOR --> GIT_CLONE
    GIT_CLONE --> TS_PARSER
    TS_PARSER --> GO_PARSER
    TS_PARSER --> PY_PARSER

    GO_PARSER --> CHUNKER
    PY_PARSER --> CHUNKER
    CHUNKER --> SEMANTIC_ANALYZER

    SEMANTIC_ANALYZER --> BATCH_PROCESSOR
    BATCH_PROCESSOR --> GEMINI_CLIENT
    GEMINI_CLIENT -.->|Retry on Failure| RETRY_LOGIC
    RETRY_LOGIC -.-> GEMINI_CLIENT

    GEMINI_CLIENT --> POSTGRES
    JOB_PROCESSOR -->|Update Job Status| POSTGRES

    %% Search Flow
    USER -->|POST /search| API_SERVER
    API_SERVER --> SEARCH_HANDLER
    SEARCH_HANDLER --> SEARCH_SERVICE
    SEARCH_SERVICE -->|Generate Query Embedding| GEMINI_CLIENT
    SEARCH_SERVICE -->|Vector Similarity Search| PGVECTOR
    PGVECTOR -->|cosine distance <=>| EMBED_TABLE
    PGVECTOR -->|Partitioned Search| EMBED_PART
    SEARCH_SERVICE -->|Retrieve Chunks| POSTGRES
    SEARCH_SERVICE -->|Format Results| SEARCH_HANDLER

    %% Data Relationships
    POSTGRES --> REPO_TABLE
    POSTGRES --> CHUNKS_TABLE
    POSTGRES --> EMBED_TABLE
    POSTGRES --> EMBED_PART
    POSTGRES --> JOBS_TABLE
    PGVECTOR -.->|Vector Operations| EMBED_TABLE
    PGVECTOR -.->|HNSW Indexing| EMBED_PART

    %% Health Monitoring
    USER -->|GET /health| HEALTH
    HEALTH -.->|Check Status| POSTGRES
    HEALTH -.->|Check Status| NATS_STREAM

    %% Error Handling
    JOB_PROCESSOR -.->|Classification| ERROR_ENTITY
    ERROR_ENTITY -.->|Aggregation| ALERT_ENTITY
    ALERT_ENTITY -.->|Notifications| USER

    %% Styling
    classDef userLayer fill:#e1f5fe
    classDef apiLayer fill:#f3e5f5
    classDef appLayer fill:#e8f5e8
    classDef domainLayer fill:#fff3e0
    classDef messageLayer fill:#fce4ec
    classDef processLayer fill:#f1f8e9
    classDef storageLayer fill:#e0f2f1
    classDef embedLayer fill:#fff8e1

    class USER,API_CLIENT userLayer
    class API_SERVER,REPO_HANDLER,SEARCH_HANDLER,HEALTH apiLayer
    class REPO_SERVICE,SEARCH_SERVICE,WORKER_SERVICE,JOB_PROCESSOR appLayer
    class REPO_ENTITY,JOB_ENTITY,ALERT_ENTITY,ERROR_ENTITY domainLayer
    class NATS_PUBLISHER,NATS_STREAM,NATS_CONSUMER messageLayer
    class GIT_CLONE,TS_PARSER,GO_PARSER,PY_PARSER,CHUNKER,SEMANTIC_ANALYZER processLayer
    class POSTGRES,PGVECTOR,REPO_TABLE,CHUNKS_TABLE,EMBED_TABLE,EMBED_PART,JOBS_TABLE storageLayer
    class GEMINI_CLIENT,BATCH_PROCESSOR,RETRY_LOGIC embedLayer
```

### Key Data Flow Stages

1. **Repository Submission**: User submits repository URL via REST API
2. **Job Creation**: Repository entity created, indexing job queued to NATS JetStream
3. **Background Processing**: Worker clones repository and parses code with Tree-sitter
4. **Semantic Chunking**: Code parsed into semantic units (functions, classes, methods)
5. **Embedding Generation**: Google Gemini API generates 768-dimensional embeddings
6. **Vector Storage**: Embeddings stored in PostgreSQL with pgvector HNSW indexing
7. **Search & Retrieval**: Vector similarity search enables semantic code search

### Processing Capabilities

- **Multi-Language Support**: Go and Python parsers with extensible architecture
- **Intelligent Chunking**: Function-level, class-level, and adaptive chunking strategies
- **Fault Tolerance**: Circuit breakers, retry logic, and comprehensive error handling
- **Scalable Processing**: Parallel batch processing with configurable worker pools
- **Performance Optimization**: HNSW indexing, connection pooling, and caching

## Prerequisites

- Go 1.24 or higher
- Docker and Docker Compose
- PostgreSQL with pgvector extension
- NATS JetStream
- Google Gemini API key
- `golangci-lint` (for development)
- `migrate` CLI tool (installed via `make install-tools`)

## Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/Anthony-Bible/codechunking.git
cd codechunking
```

### 2. Set up environment variables

```bash
cp .env.example .env
# Edit .env and add your Gemini API key
```

### 3. Start development environment

```bash
# Start Docker services (PostgreSQL, NATS)
make dev

# Run database migrations
make migrate-up

# Start API server
make dev-api

# In another terminal, start worker
make dev-worker
```

## Installation

### Using Go

```bash
go install github.com/Anthony-Bible/codechunking@latest
```

### Using Docker

```bash
docker pull yourusername/codechunking:latest
```

### From source

```bash
git clone https://github.com/Anthony-Bible/codechunking.git
cd codechunking
make build
```

## Usage

### CLI Commands

```bash
# Show help
codechunking --help

# Start API server
codechunking api --config configs/config.dev.yaml

# Start worker
codechunking worker --concurrency 10

# Run migrations
codechunking migrate up

# Show version
codechunking version
```

### API Endpoints

#### Index a repository
```bash
curl -X POST http://localhost:8080/api/v1/repositories \
  -H "Content-Type: application/json" \
  -d '{"url": "https://github.com/example/repo"}'
```

#### Health check with monitoring
```bash
curl http://localhost:8080/health
# Returns health status with NATS monitoring, response time metrics
```

#### Search code semantically âœ¨
```bash
curl -X POST http://localhost:8080/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "implement authentication middleware",
    "limit": 10,
    "similarity_threshold": 0.7,
    "languages": ["go"],
    "sort": "similarity:desc"
  }'
```

## Configuration

Configuration can be provided through:
1. Configuration files (YAML)
2. Environment variables (prefix: `CODECHUNK_`)
3. Command-line flags

Priority: Flags > Environment > Config File > Defaults

### Configuration Files

- `configs/config.yaml` - Base configuration
- `configs/config.dev.yaml` - Development overrides
- `configs/config.prod.yaml` - Production overrides

### Environment Variables

```bash
export CODECHUNK_DATABASE_USER=myuser
export CODECHUNK_DATABASE_PASSWORD=mypass
export CODECHUNK_GEMINI_API_KEY=your-api-key
export CODECHUNK_LOG_LEVEL=debug
```

## Development

### Project Structure

```
codechunking/
â”œâ”€â”€ cmd/                       # CLI entry points
â”‚   â”œâ”€â”€ codechunking/         # Main CLI
â”‚   â””â”€â”€ commands/             # Cobra commands
â”œâ”€â”€ internal/                 # Private application code
â”‚   â”œâ”€â”€ domain/              # Business entities
â”‚   â”‚   â”œâ”€â”€ entity/         # Domain entities
â”‚   â”‚   â”œâ”€â”€ valueobject/    # Value objects
â”‚   â”‚   â””â”€â”€ service/        # Domain services
â”‚   â”œâ”€â”€ application/         # Application layer
â”‚   â”‚   â”œâ”€â”€ command/        # Command handlers
â”‚   â”‚   â”œâ”€â”€ query/          # Query handlers
â”‚   â”‚   â””â”€â”€ dto/            # Data transfer objects
â”‚   â”œâ”€â”€ port/               # Port interfaces
â”‚   â”‚   â”œâ”€â”€ inbound/        # Driving ports
â”‚   â”‚   â””â”€â”€ outbound/       # Driven ports
â”‚   â”œâ”€â”€ adapter/            # Adapter implementations
â”‚   â”‚   â”œâ”€â”€ inbound/        # API, Worker adapters
â”‚   â”‚   â””â”€â”€ outbound/       # Database, NATS, etc.
â”‚   â””â”€â”€ config/             # Configuration
â”œâ”€â”€ pkg/                    # Public packages
â”œâ”€â”€ migrations/             # Database migrations
â”œâ”€â”€ configs/               # Configuration files
â”œâ”€â”€ docker/                # Docker files
â”œâ”€â”€ scripts/               # Utility scripts
â””â”€â”€ tests/                 # Integration tests
```

### Running Tests

```bash
# Unit tests
make test

# Integration tests
make test-integration

# Coverage report
make test-coverage

# All tests
make test-all
```

### Linting and Formatting

```bash
# Run linter
make lint

# Format code
make fmt

# Run go vet
make vet
```

### Database Migrations

```bash
# Create new migration
make migrate-create name=add_new_table

# Apply migrations
make migrate-up

# Rollback migrations
make migrate-down
```

## Deployment

### Docker Compose (Development)

```bash
docker-compose up -d
```

This brings up PostgreSQL (with pgvector), NATS (with monitoring), the API on `localhost:8080`, and the worker.

Quick verify:

```bash
curl http://localhost:8080/health
curl -X POST http://localhost:8080/repositories \
  -H "Content-Type: application/json" \
  -d '{"url": "https://github.com/example/repo", "name": "example"}'
```

Notes:
- The app reads env via `CODECHUNK_` variables. Compose sets DB and NATS. Add `CODECHUNK_GEMINI_API_KEY` in `docker-compose.yml` if enabling embeddings/search.
- Routes are mounted at `/`, e.g. `/repositories` (no `/api/v1` prefix in current code).

### Kubernetes (Production)

```bash
kubectl apply -f k8s/
```

### Environment Variables for Production

- `CODECHUNK_DATABASE_HOST`
- `CODECHUNK_DATABASE_USER`
- `CODECHUNK_DATABASE_PASSWORD`
- `CODECHUNK_NATS_URL`
- `CODECHUNK_GEMINI_API_KEY`

## Monitoring

### Health Checks
Production-ready health monitoring with comprehensive dependency checking:

```bash
curl http://localhost:8080/health
```

Returns detailed health information including:
- **Database connectivity** with connection pool status
- **NATS JetStream** availability and performance metrics
- **Circuit breaker** status and connection stability
- **Response time tracking** with 23.5Âµs average response time
- **Caching layer** with 5-second TTL for performance

### Metrics and Observability
The system exposes comprehensive monitoring:

- **API Health**: `http://localhost:8080/health` (with custom headers)
- **NATS Monitoring**: `http://localhost:8222` (connection metrics)
- **Structured Logging**: JSON format with correlation IDs
- **Performance Metrics**: Request duration, throughput, error rates

## Performance

### Current Performance Metrics
- **NATS Throughput**: 305,358+ messages/second
- **Health Check Response**: 23.5Âµs average response time
- **Health Check Caching**: 5-second TTL with memory optimization
- **Database Connection Pooling**: Optimized for concurrent operations

### Performance Tuning

### PostgreSQL with pgvector

Optimize HNSW index parameters in migrations:
```sql
CREATE INDEX ON embeddings 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

### Worker Concurrency

Adjust worker concurrency based on your system:
```bash
codechunking worker --concurrency 20
```

### Connection Pooling

Configure database connection pool in config:
```yaml
database:
  max_connections: 100
  max_idle_connections: 20
```

## Troubleshooting

### Common Issues

1. **pgvector extension not found**
   ```bash
   # Use the pgvector Docker image
   docker pull pgvector/pgvector:pg16
   ```

2. **NATS connection refused**
   ```bash
   # Check NATS is running
   docker-compose ps
   nats -s nats://localhost:4222 server check
   ```

3. **Migration failures**
   ```bash
   # Check database connection
   make psql
   # Manually check migration status
   SELECT * FROM schema_migrations;
   ```

4. **Health check issues**
   ```bash
   # Check NATS health with detailed diagnostics
   curl -v http://localhost:8080/health
   # Look for X-NATS-Connection-Status and X-JetStream-Enabled headers
   ```

5. **Security validation errors**
   ```bash
   # Check logs for XSS/SQL injection detection
   # Review request validation in structured logs with correlation IDs
   ```

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines

- **TDD Methodology**: Use red-green-refactor cycle with specialized agents
- Follow Go best practices and idioms
- Maintain test coverage above 80% (currently 100+ passing tests across all layers)
- Update documentation for new features
- **Use conventional commit messages** (required)
- Run `make lint` and `make fmt` before committing
- Keep files under 1000 lines for readability (preferably under 500)
- Always use timeout for tests: `go test ./... -timeout 10s`

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- [tree-sitter](https://tree-sitter.github.io/) for code parsing
- [pgvector](https://github.com/pgvector/pgvector) for vector similarity search
- [NATS](https://nats.io/) for messaging
- [Google Gemini](https://ai.google.dev/) for embeddings
- [Cobra](https://github.com/spf13/cobra) for CLI
- [Viper](https://github.com/spf13/viper) for configuration

## Support

For issues and questions:
- GitHub Issues: [github.com/yourusername/codechunking/issues](https://github.com/yourusername/codechunking/issues)
- Documentation: [docs.codechunking.io](https://docs.codechunking.io)

## Documentation

For detailed documentation, see the `/docs` directory and the project wiki:

- **[Project Wiki](wiki/)**: Comprehensive configuration reference and guides
  - [Configuration Overview](wiki/configuration/Configuration.md)
  - [API Configuration](wiki/configuration/API-Configuration.md)
  - [Database Configuration](wiki/configuration/Database-Configuration.md)
  - [NATS Configuration](wiki/configuration/NATS-Configuration.md)
  - [Worker Configuration](wiki/configuration/Worker-Configuration.md)
  - [Gemini Configuration](wiki/configuration/Gemini-Configuration.md)
  - [Logging Configuration](wiki/configuration/Logging-Configuration.md)
  - [Git Configuration](wiki/configuration/Git-Configuration.md)
- **[API Documentation](docs/api.md)**: Complete REST API reference
- **[Development Guide](docs/development.md)**: Enhanced setup and development workflow
- **[Deployment Guide](docs/deployment.md)**: Production deployment and scaling

### Working with the Wiki

The wiki is included as a Git submodule. To clone the repository with the wiki:

```bash
git clone --recurse-submodules https://github.com/yourusername/codechunking.git
```

If you've already cloned the repository, initialize and update the submodule:

```bash
git submodule init
git submodule update
```

To update the wiki to the latest version:

```bash
git submodule update --remote wiki
```

## Current Status

ðŸŽ‰ **MVP COMPLETE** - All core phases delivered (100+ passing tests):
- âœ… **Phase 2**: Production-ready API with repository management
- âœ… **Phase 3**: Asynchronous processing with NATS JetStream
- âœ… **Phase 4**: Intelligent code chunking with tree-sitter
- âœ… **Phase 5**: Vector embeddings with Google Gemini
- âœ… **Phase 6**: Vector storage and retrieval with pgvector
- âœ… **Phase 7.1**: **Semantic search API** with natural language queries âœ¨

**Key MVP Features Delivered:**
- Complete semantic code search with POST /search endpoint
- Advanced filtering (repository, language, file type, similarity threshold)
- Pagination and sorting capabilities
- Comprehensive TDD implementation with 50+ passing tests
- Production-ready error handling and validation
- Full OpenAPI documentation

## Post-MVP Roadmap

- [ ] Support for more programming languages (Python, JavaScript, etc.)
- [ ] Incremental repository updates and webhooks
- [ ] Web UI for search interface
- [ ] GitHub/GitLab webhooks for auto-indexing
- [ ] Multi-model embedding support
- [ ] Distributed worker scaling
- [ ] Query result caching and performance optimization
- [ ] Fine-tuned ranking algorithms and hybrid search
