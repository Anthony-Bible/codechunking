# Base configuration for CodeChunking
api:
  host: 0.0.0.0
  port: 8080
  read_timeout: 10s
  write_timeout: 10s

worker:
  concurrency: 5
  queue_group: workers
  job_timeout: 30m  # Overall timeout for complete job processing
                    # Must be larger than (gemini.timeout Ã— max_chunks_per_repo)
                    # to allow all embeddings to complete

database:
  host: localhost
  port: 5432
  name: codechunking
  sslmode: disable
  max_connections: 25
  max_idle_connections: 5

nats:
  url: nats://localhost:4222
  max_reconnects: 5
  reconnect_wait: 2s

search:
  iterative_scan_mode: relaxed_order  # pgvector 0.8.0+ iterative scanning mode
                                       # Options: "off", "strict_order", "relaxed_order"
                                       # relaxed_order recommended for best recall/performance

gemini:
  model: gemini-embedding-001
  max_retries: 3
  timeout: 120s  # Per-request timeout for embedding generation
                 # Applied to each individual embedding API call
                 # Should be less than worker job_timeout
  batch:
    enabled: true
    input_dir: /tmp/batch_embeddings/input
    output_dir: /tmp/batch_embeddings/output
    poll_interval: 5s
    max_wait_time: 30m

# Batch processing configuration for enhanced embedding generation
batch_processing:
  # Enable queue-based batch processing alongside individual processing
  enabled: true
  # Threshold for switching to batch processing (chunks > this value use batch)
  threshold_chunks: 50
  # Batch size configuration by priority level
  batch_sizes:
    # Real-time priority: 1-25 requests (lowest latency)
    realtime:
      min: 1
      max: 25
      timeout: 5m
    # Interactive priority: 5-50 requests (responsive UI)
    interactive:
      min: 5
      max: 50
      timeout: 10m
    # Background priority: 50-200 requests (balanced for job processing)
    background:
      min: 50
      max: 200
      timeout: 30m
    # Batch priority: 100-500 requests (maximum efficiency)
    batch:
      min: 100
      max: 500
      timeout: 60m
  # Fallback configuration
  fallback_to_sequential: true  # Fall back to individual processing on batch failures
  queue_limits:
    max_queue_size: 10000       # Maximum requests in queue
    max_wait_time: 30m          # Maximum time waiting in queue
  # Default priority for job processing
  default_priority: "background"
  # Test embeddings configuration (for development/testing only)
  use_test_embeddings: false
  # Batch chunking strategy parameters
  max_batch_size: 500           # Maximum chunks per API batch
  initial_backoff: 30s          # Initial backoff delay for retries
  max_backoff: 300s             # Maximum backoff delay
  max_retries: 3                # Maximum retry attempts per batch
  enable_batch_chunking: true   # Enable/disable batch chunking

  # Batch Submitter configuration (rate-limit-aware submission)
  submitter_poll_interval: 5s         # How often to check for pending submissions
  max_concurrent_submissions: 1       # Max parallel Gemini API submissions (1 = safest)
  submission_initial_backoff: 1m      # Initial backoff on rate limit
  submission_max_backoff: 30m         # Maximum backoff duration
  max_submission_attempts: 10         # Max retry attempts per batch submission

  # Token counting configuration
  token_counting:
    enabled: true                    # Enable token counting
    mode: "all"                      # Mode: "all", "sample", or "on_demand"
    sample_percent: 10               # Percentage of chunks to sample (for "sample" mode)
    max_tokens_per_chunk: 8192       # Maximum tokens per chunk (Gemini embedding model limit)

log:
  level: info
  format: json
